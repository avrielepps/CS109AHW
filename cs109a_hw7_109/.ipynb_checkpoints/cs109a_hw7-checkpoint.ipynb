{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science \n",
    "\n",
    "## Homework 7: Classification with Logistic Regression, LDA/QDA, and Trees\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the [instructions given in Canvas](https://canvas.harvard.edu/courses/42693/pages/homework-policies-and-submission-instructions).\n",
    "\n",
    "- If needed, clarifications will be posted on Piazza.\n",
    "\n",
    "- This homework can be submitted in pairs.\n",
    "\n",
    "- If you submit individually but you have worked with someone, please include the name of your **one** partner below. \n",
    "\n",
    "\n",
    "**Name of the person you have worked with goes here:**\n",
    "<br><BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avriel Epps, Erin Williams\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 1 [20 pts]: Overview of Multiclass Thyroid Classification </b></div>\n",
    "\n",
    "In this problem set you will build a model for diagnosing disorders in a patient's thyroid gland. Given the results of medical tests on a patient, the task is to classify the patient either as:\n",
    "- *normal* (class 1)\n",
    "- having *hyperthyroidism* (class 2)\n",
    "- or having *hypothyroidism* (class 3). \n",
    "\n",
    "The data set is provided in the file `dataset_hw7.csv`. Columns 1-2 contain biomarkers for a patient (predictors):\n",
    "- Biomarker 1: (Logarithm of) level of basal thyroid-stimulating hormone (TSH) as measured by radioimmuno assay\n",
    "- Biomarker 2: (Logarithm of) maximal absolute difference of TSH value after injection of 200 micro grams of thyrotropin-releasing hormone as compared to the basal value.\n",
    "\n",
    "The last column contains the diagnosis for the patient from a medical expert. This data set was [obtained from the UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Thyroid+Disease); for this assignment we chose two predictors so we can visualize the decision boundaries.\n",
    "\n",
    "Notice that unlike previous exercises, the task at hand is a 3-class classification problem. We will explore different methods for multiclass classification.\n",
    "\n",
    "For most of this problem set, we'll measure overall classification accuracy as the fraction of observations classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**1.1** Load the data and examine its structure. How many instances of each class are there in our dataset? In particular, what is the ratio of the number of observations in class 2 (hyperthyroidism) to the number of observations in class 3 (hypothyroidism)? We'll refer to this as the *hyper-to-hypo ratio*.\n",
    "\n",
    "**1.2**: We're going to split this data into a 50% training set and a 50% test set. But since our dataset is small, we need to make sure we do it correctly. Let's see what happens when we *don't* split correctly: for each of 100 different random splits of the data into 50% train and 50% test, compute the hyper-to-hypo for the observations end up in the training set. Plot the distribution of the hyper-to-hypo ratio; on your plot, also mark the hyper-to-hypo ratio that you found in the full dataset. Discuss how representative the training and test sets are likely to be if we were to have selected one of these random splits.\n",
    "\n",
    "**1.3** Now, we'll use the `stratify` option to split the data in such a way that the relative class frequencies are preserved (the code is provided). Make a table showing how many observations of each class ended up in your training and test sets. Verify that the hyper-hypo ratio is roughly the same in both sets.\n",
    "\n",
    "**1.4** Provide the scatterplot of the predictors in the (training) data in a way that clearly indicates which class each observation belongs to.\n",
    "\n",
    "**1.5**: When we first start working with a dataset or algorithm, it's typically a good idea to figure out what *baselines* we might compare our results to. For regression, we always compared against a baseline of predicting the mean (in computing $R^2$). For classification, a simple baseline is always predicting the *most common class*. What \"baseline\" accuracy can we achieve on the thyroid classification problem by always predicting the most common class? Assign the result to `baseline_accuracy` so we can use it later. (**note: don't look at the test set until instructed**)\n",
    "\n",
    "**1.6** Make a decision function to separate these samples using no library functions; just write out your logic by hand. Your manual classifier doesn't need to be well-tuned (we'll be exploring algorithms to do that!); it only needs to (1) predict each class at least once, and (2) achieve an accuracy at least 10% greater accurate than predicting the most likely class. Use the `overlay_decision_boundaries` function provided above to overlay the decision boundaries of your function on the training set. (Note that the function modifies an existing plot, so call it after plotting your points.)\n",
    "\n",
    "Based on your exploration, do you think a linear classifier (i.e., a classifier where all decision boundaries are line segments) could achieve above 85% accuracy on this dataset? Could a non-linear classifier do better? What characteristics of the data lead you to these conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Biomarker 1</th>\n",
       "      <th>Biomarker 2</th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>215.000000</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>215.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.414441</td>\n",
       "      <td>0.303155</td>\n",
       "      <td>1.441860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.888106</td>\n",
       "      <td>2.174369</td>\n",
       "      <td>0.726737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.302485</td>\n",
       "      <td>-11.512925</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.510809</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.262372</td>\n",
       "      <td>0.693152</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.530634</td>\n",
       "      <td>1.410989</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.032469</td>\n",
       "      <td>4.030695</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Biomarker 1  Biomarker 2   Diagnosis\n",
       "count   215.000000   215.000000  215.000000\n",
       "mean      0.414441     0.303155    1.441860\n",
       "std       0.888106     2.174369    0.726737\n",
       "min      -2.302485   -11.512925    1.000000\n",
       "25%       0.000010    -0.510809    1.000000\n",
       "50%       0.262372     0.693152    1.000000\n",
       "75%       0.530634     1.410989    2.000000\n",
       "max       4.032469     4.030695    3.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "data = pd.read_csv('dataset_HW7.csv')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    150\n",
       "2     35\n",
       "3     30\n",
       "Name: Diagnosis, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "(data['Diagnosis']).value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: There are 150 instances of normal thyroid levels, 35 instances of hyperthroidism, and 30 cases of hypothyroidism. The hyper-to-hypo ratio is 7:6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2**We're going to split this data into a 50% training set and a 50% test set. But since our dataset is small, we need to make sure we do it correctly. Let's see what happens when we *don't* split correctly: for each of 100 different random splits of the data into 50% train and 50% test, compute the hyper-to-hypo for the observations end up in the training set. Plot the distribution of the hyper-to-hypo ratio; on your plot, also mark the hyper-to-hypo ratio that you found in the full dataset. Discuss how representative the training and test sets are likely to be if we were to have selected one of these random splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratio_list = []\n",
    "for i in range (0, 100):\n",
    "    data_train, data_test = train_test_split(data, test_size=.5)\n",
    "    counts_train = (data_train['Diagnosis']).value_counts()\n",
    "    train_hyper = counts_train[2]\n",
    "    train_hypo = counts_train[3]\n",
    "    ratio = train_hyper/train_hypo\n",
    "    ratio_list.append(ratio)\n",
    "\n",
    "x=np.linspace(0, 100, 100)\n",
    "y=0*x + (7/6)\n",
    "\n",
    "len(ratio_list)\n",
    "    \n",
    "plt.scatter(x,ratio_list)\n",
    "plt.plot(y, color = 'r', alpha = 0.8)\n",
    "plt.title(\"Hyper-to-Hypo Ratio\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(df, test_size=.5, stratify=df.Diagnosis, random_state=99);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_decision_boundary(ax, model, colors=None, nx=200, ny=200, desaturate=.5):\n",
    "    \"\"\"\n",
    "    A function that visualizes the decision boundaries of a classifier.\n",
    "    \n",
    "    ax: Matplotlib Axes to plot on\n",
    "    model: Classifier (has a `.predict` method)\n",
    "    X: feature vectors\n",
    "    y: ground-truth classes\n",
    "    colors: list of colors to use. Use color colors[i] for class i.\n",
    "    nx, ny: number of mesh points to evaluated the classifier on\n",
    "    desaturate: how much to desaturate each of the colors (for better contrast with the sample points)\n",
    "    \"\"\"\n",
    "    # Create mesh\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(xmin, xmax, nx),\n",
    "        np.linspace(ymin, ymax, ny))\n",
    "    X = np.c_[xx.flatten(), yy.flatten()]\n",
    "\n",
    "    # Predict on mesh of points\n",
    "    if hasattr(model, 'predict'):\n",
    "        model = model.predict\n",
    "    y = model(X)\n",
    "    y = y.reshape((nx, ny))\n",
    "\n",
    "    # Generate colormap.\n",
    "    if colors is None:\n",
    "        colors = sns.utils.get_color_cycle()\n",
    "        y -= y.min() # If first class is not 0, shift.\n",
    "    assert np.max(y) <= len(colors)\n",
    "    colors = [sns.utils.desaturate(color, desaturate) for color in colors]\n",
    "    cmap = matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "    # Plot decision surface\n",
    "    ax.pcolormesh(xx, yy, y, zorder=-2, cmap=cmap, norm=matplotlib.colors.NoNorm(), vmin=0, vmax=y.max() + 1)\n",
    "    xx = xx.reshape(nx, ny)\n",
    "    yy = yy.reshape(nx, ny)\n",
    "#     ax.contourf(xx, yy, y, cmap=cmap, vmin=0, vmax=3)\n",
    "    ax.contour(xx, yy, y, colors=\"black\", linewidths=1, zorder=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the following function:\n",
    "def predict_manual_one_sample(x):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_manual(X):\n",
    "    return np.array([predict_manual_one_sample(x) for x in X])\n",
    "\n",
    "manual_predictions = predict_manual(X)\n",
    "accuracy = accuracy_score(y, manual_predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert accuracy >= (baseline_accuracy * 1.10), \"Accuracy too low\"\n",
    "assert all(np.sum(manual_predictions == i) > 0 for i in [1, 2, 3]), \"Should predict each class at least once.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 2 [20 pts]: Multiclass Logistic Regression</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**2.1** Fit two one-vs-rest logistic regression models using sklearn. For the first model, use the train dataset as-is (so the decision boundaries will be linear); for the second model, also include quadratic and interaction terms. For both models, use $L_2$ regularization, tuning the regularization parameter using 5-fold cross-validation. \n",
    "\n",
    "For each model, make a plot of the training data with the decision boundaries overlayed.\n",
    "\n",
    "**2.2** Interpret the decision boundaries:\n",
    "- Do these decision boundaries make sense?\n",
    "- What does adding quadratic and interaction features do to the shape of the decision boundaries? Why?\n",
    "- How do the different models treat regions where there are few samples? How do they classify such samples?\n",
    "\n",
    "**2.3** Compare the performance of the two logistic regression models above using 5-fold cross-validation. Which model performs best? How confident are you about this conclusion? Does the inclusion of the polynomial terms in logistic regression yield better accuracy compared to the model with only linear terms? Why do you suspect it is better or worse?\n",
    "\n",
    "*Hint*: You may use the `cross_val_score` function for cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint*: You should use `LogisticRegressionCV`. For the model with quadratic and interaction terms, use the following Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_logreg_estimator = make_pipeline(\n",
    "    PolynomialFeatures(degree=2, include_bias=False),\n",
    "    LogisticRegressionCV(multi_class=\"multinomial\"))\n",
    "\n",
    "# Note that you can access the logistic regression classifier itself by\n",
    "# polynomial_logreg_estimator.named_steps['logisticregressioncv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 3 [20 pts]: Discriminant Analysis</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**3.1** Consider the following synthetic dataset with two classes. A green star marks a test observation; which class do you think it belongs to? How would LDA classify that observation? How would QDA? Explain your reasoning.\n",
    "\n",
    "**3.2** Now let's return to the thyroid dataset. Make a table of the total variance of each class for each biomarker.\n",
    "\n",
    "**3.3** Fit LDA and QDA on the thyroid data, and plot the decision boundaries. Comment on how the decision boundaries differ. How does the difference in decision boundaries relate to characteristics of the data, such as the table you computed above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blobs, y_blobs = make_blobs(centers=[[0., 0.], [1., 0.]], cluster_std=[[.4, .1], [.1, 20.]], random_state=0)\n",
    "plt.scatter(X_blobs[y_blobs==0][:,0], X_blobs[y_blobs==0][:,1], label=\"Class 0\")\n",
    "plt.scatter(X_blobs[y_blobs==1][:,0], X_blobs[y_blobs==1][:,1], label=\"Class 1\")\n",
    "plt.scatter([.75], [0.], color=\"green\", marker=\"*\", s=150, label=\"Test observation\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class='exercise'> <b> Question 4 [20 pts]: Fit Decision Trees </b> </div> \n",
    "\n",
    "\n",
    "We next try out decision trees for thyroid classification. For the following questions, you should use the *Gini* index as the splitting criterion while fitting the decision tree. \n",
    "\n",
    "*Hint:* You should use the `DecisionTreeClassifier` class to fit a decision tree classifier and the `max_depth` attribute to set the tree depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**4.1**. Fit a decision tree model to the thyroid data set with (maximum) tree depths 2, 3, ..., 10. Make plots of the accuracy as a function of the maximum tree depth, on the training set and the mean score on the validation sets for 5-fold CV. Is there a depth at which the fitted decision tree model achieves near-perfect classification on the training set? If so, what can you say about how this tree will generalize? Which hyperparameter setting gives the best cross-validation performance?\n",
    "\n",
    "**4.2**: Visualize the decision boundaries of the best decision tree you just fit. How are the shapes of the decision boundaries for this model different from the other methods we have seen so far? Given an explanation for your observation.\n",
    "\n",
    "**4.3** Explain *in words* how the best fitted model diagnoses 'hypothyroidism' for a new patient. You can use the code below to examine the structure of the best decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Entirely optional note:* You can also generate a visual representation using the `export_graphviz`. However, viewing the generated GraphViz file requires additional steps. One approach is to paste the generated graphviz file in the text box at http://www.webgraphviz.com/. Alternatively, you can run GraphViz on your own computer, but you may need to install some additional software. Refer to the [Decision Tree section of the sklearn user guide](http://scikit-learn.org/stable/modules/tree.html#classification) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is adapted from\n",
    "# http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
    "def show_tree_structure(clf):\n",
    "    tree = clf.tree_\n",
    "\n",
    "    n_nodes = tree.node_count\n",
    "    children_left = tree.children_left\n",
    "    children_right = tree.children_right\n",
    "    feature = tree.feature\n",
    "    threshold = tree.threshold\n",
    "\n",
    "    # The tree structure can be traversed to compute various properties such\n",
    "    # as the depth of each node and whether or not it is a leaf.\n",
    "    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "    stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "    while len(stack) > 0:\n",
    "        node_id, parent_depth = stack.pop()\n",
    "        node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "        # If we have a test node\n",
    "        if (children_left[node_id] != children_right[node_id]):\n",
    "            stack.append((children_left[node_id], parent_depth + 1))\n",
    "            stack.append((children_right[node_id], parent_depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "\n",
    "    print(f\"The binary tree structure has {n_nodes} nodes:\\n\")\n",
    "    \n",
    "    for i in range(n_nodes):\n",
    "        indent = node_depth[i] * \"  \"\n",
    "        if is_leaves[i]:\n",
    "            prediction = clf.classes_[np.argmax(tree.value[i])]\n",
    "            print(f\"{indent}node {i}: predict class {prediction}\")\n",
    "        else:\n",
    "            print(\"{}node {}: if X[:, {}] <= {:.3f} then go to node {}, else go to node {}\".format(\n",
    "                indent, i, feature[i], threshold[i], children_left[i], children_right[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 5 [18 pts]: k-NN and Model comparison </b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now seen six different ways of fitting a classification model: **linear logistic regression**, **logistic regression with polynomial terms**, **LDA**, **QDA**, **decision trees**, and in this problem we'll add **k-NN**. Which of these methods should we use in practice for this problem? To answer this question, we now compare and contrast these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**5.1** Fit a k-NN classifier with uniform weighting to the training set. Use 5-fold CV to pick the best $k$.\n",
    "\n",
    "*Hint: Use `KNeighborsClassifier` and `cross_val_score`.*\n",
    "\n",
    "**5.2** Plot the decision boundaries for each of the following models that you fit above. For models with hyperparameters, use the values you chose using cross-validation.\n",
    "- Logistic Regression (linear)\n",
    "- Logistic Regression (polynomial)\n",
    "- Linear Discriminant Analysis \n",
    "- Quadratic Discriminant Analysis\n",
    "- Decision Tree\n",
    "- k-NN\n",
    "\n",
    "Comment on the difference in the decision boundaries between the following pairs of models. Why does this difference make sense given how the model works?\n",
    "- Linear logistic regression; LDA\n",
    "- Quadratic logistic regression; QDA.\n",
    "- k-NN and whichever other model has the most complex decision boundaries\n",
    "\n",
    "**5.3** Describe how each model classifies an observation from the test set in one short sentence for each (assume that the model is already fit). For example, for the linear regression classifier you critiqued in hw5, you might write: \"It classifies the observation as class 1 if the dot product of the feature vector with the the model coefficients (with constant added) exceeds 0.5.\"\n",
    "\n",
    "- Logistic Regression (One-vs-Rest)\n",
    "- Linear Discriminant Analysis\n",
    "- Quadratic Discriminant Analysis\n",
    "- k-Nearest-Neighbors Classifier\n",
    "- Decision Tree\n",
    "\n",
    "**5.4** Estimate the validation accuracy for each of the models. Summarize your results in a graph or table. (Note: for some models you have already run these computations; it's ok to redo them here if it makes your code cleaner.)\n",
    "\n",
    "**5.5** Based on everything you've found in this question so far, which model would you expect to perform best on our test data? \n",
    "\n",
    "Now evaluate each fitted model's performance on the test set. Also, plot the same decision boundaries as above, but now showing the test set. How did the overall performance compare with your performance estimates above? Which model actually performed best? Why do you think this is the case?\n",
    "\n",
    "**5.6**. Compare and contrast the six models based on each of the following criteria (a supporting table to summarize your  thoughts can be helpful):\n",
    "  - Classification performance\n",
    "  - Complexity of decision boundary\n",
    "  - Memory storage\n",
    "  - Interpretability\n",
    "\n",
    "If you were a clinician who had to use the classifier to diagnose thyroid disorders in patients, which among the six methods would you be most comfortable in using? Justify your choice in terms of at least 3 different aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 6: [2 pts] Including an 'abstain' option </b></div>\n",
    "**Note this question is only worth 2 pts. **\n",
    "\n",
    "One of the reasons a hospital might be hesitant to use your thyroid classification model is that a misdiagnosis by the model on a patient can sometimes prove to be very costly (e.g. if the patient were to file a law suit seeking a compensation for damages). One way to mitigate this concern is to allow the model to 'abstain' from making a prediction: whenever it is uncertain about the diagnosis for a patient. However, when the model abstains from making a prediction, the hospital will have to forward the patient to a thyroid specialist (i.e. an endocrinologist), which would incur additional cost.  How could one design a thyroid classification model with an abstain option, such that the cost to the hospital is minimized?\n",
    "\n",
    "*Hint:* Think of ways to build on top of the logistic regression model and have it abstain on patients who are difficult to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**6.1** More specifically, suppose the cost incurred by a hospital when a model mis-predicts on a patient is \\$5000, and the cost incurred when the model abstains from making a prediction is \\$1000. What is the average cost per patient for the OvR logistic regression model (without quadratic or interaction terms) from Question 2? Note that this needs to be evaluated on the patients in the test set. \n",
    "\n",
    "**6.2** Design a classification strategy (into the 3 groups plus the *abstain* group) that has as low cost as possible per patient (certainly lower cost per patient than the logistic regression model).   Give a justification for your approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "374px",
    "width": "564px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
