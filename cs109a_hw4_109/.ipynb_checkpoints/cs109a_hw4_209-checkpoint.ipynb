{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science: \n",
    "\n",
    "## Homework 4  AC 209 : Regularization\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader\n",
    "\n",
    "Names of people you have worked with goes here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL FOR FORMAT\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 1 [12 pts] </b> </div>\n",
    "\n",
    "Ridge and LASSO regularizations are powerful tools that not only increase generalization, but also expand the range of problems that we can solve. We will study this statement in this question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1** Let $X\\in \\mathbb{R}^{n\\times p}$ be a matrix of observations, where each row corresponds an observation and each column corresponds to a predictor. Now consider the case $p > n$: explain why there is no unique solution to the OLS estimator. \n",
    "\n",
    "**5.2**  Now consider the Ridge formulation. Show that finding the ridge estimator is equivalent to solving an OLS problem after adding p dummy observations with their X value equal to $\\lambda$ at the j-th component and zero everywhere else, and their Y value set to zero. In a nutshell, show that the ridge estimator can be found by getting the least squares estimator for the augmented problem:\n",
    "\n",
    "$$X^* = \\begin{bmatrix} X \\\\ \\sqrt{\\lambda}I \\end{bmatrix}$$\n",
    "\n",
    "$$Y^* = \\begin{bmatrix} Y \\\\ \\textbf{0} \\end{bmatrix}$$\n",
    "\n",
    "**5.3** Can we now solve the $p > n$ situation? Explain why.\n",
    "\n",
    "**5.4** Take a look at the LASSO estimator expression that we derived when $X^TX=I$. What needs to happen for LASSO to nullify  $\\beta_i$?\n",
    "\n",
    "**5.5**  Can LASSO be used when $p>n$? What important consideration, related to the number of predictors that LASSO chooses, do we have to keep in mind in that case?\n",
    "\n",
    "**5.6** Ridge and LASSO still have room for improvement. List two limitations of Ridge, and two limitations of LASSO.\n",
    "\n",
    "**5.7** Review the class slides and answer the following questions: When is Ridge preferred? When is LASSO preferred? When is Elastic Net preferred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1** Let $X\\in \\mathbb{R}^{n\\times p}$ be a matrix of observations, where each row corresponds an observation and each column corresponds to a predictor. Now consider the case $p > n$: explain why there is no unique solution to the OLS estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have more collumns than you have rows, you have more unknowns than you can solve for, which is why there is no unique solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2**  Now consider the Ridge formulation. Show that finding the ridge estimator is equivalent to solving an OLS problem after adding p dummy observations with their X value equal to $\\lambda$ at the j-th component and zero everywhere else, and their Y value set to zero. In a nutshell, show that the ridge estimator can be found by getting the least squares estimator for the augmented problem:\n",
    "\n",
    "$$X^* = \\begin{bmatrix} X \\\\ \\sqrt{\\lambda}I \\end{bmatrix}$$\n",
    "\n",
    "$$Y^* = \\begin{bmatrix} Y \\\\ \\textbf{0} \\end{bmatrix}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3** Can we now solve the $p > n$ situation? Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve the $p > n$ situation now because we've created enough dummy observations so that $p = n$. As such, you're able to invert the gram matrix now so that you can achieve a unique solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4** Take a look at the LASSO estimator expression that we derived when $X^TX=I$. What needs to happen for LASSO to nullify  $\\beta_i$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.5**  Can LASSO be used when $p>n$? What important consideration, related to the number of predictors that LASSO chooses, do we have to keep in mind in that case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. LASSO can't be used when $p > n$. The thing to consider is that, while LASSO is good for feature selection, LASSO doesn't work well when $p > n$, because of the nature of the optimization problem that LASSO tries to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.6 ** Ridge and LASSO still have room for improvement. List two limitations of Ridge, and two limitations of LASSO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Limitations:\n",
    "1. Not as good for feature selection, because it penalizes large values more than smaller values. Therefore, it has no incentive to remove parameters that are sufficiently small. In short, Ridge performs shrinkage, but not variable selection.\n",
    "2. Because Ridge doesn't remove variables, it impaires interpretability of the model. We don't know which features are important and which ones are not, because Ridge keeps them all.\n",
    "\n",
    "LASSO Limitations:\n",
    "1. LASSO doesn't work when $p > n$, as discussed above.\n",
    "2. In LASSO, when two features are collinear, it can't determine which feature is best for the model, and will just remove one of them arbitrarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.7** Review the class slides and answer the following questions: When is Ridge preferred? When is LASSO preferred? When is Elastic Net preferred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge is preferred when you know the parameters you want and want to add larger penalties to larger coefficients. LASSO is preferred when you want feature selection. And, if $p > n$ and you want feature selection, Elastic Net is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 6 [12pts]</b></div>\n",
    "\n",
    "We want to analyze the behavior of our estimators in cases where p > n. We will generate dummy regression problems for this analysis, so that we have full control on the properties of the problem. Sklearn provides an easy to use function to generate regression problems: `sklearn.datasets.make_regression`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1** Use the provided notebook cell to to build a dataset with 500 samples, 2500 features, 100 informative features and a noise sd of 10.0. The function will return the true coefficients in `true_coef`. Intercepts are not generated, so do not fit them in your regressions. Fit LinearRegression, LassoCV, RidgeCV and ElasticNetCV estimators on the traininig set with 5-fold crossvalidation.\n",
    "\n",
    "Test 100 lambda values from 0.01 to 1000, in logscale. For Elastic Net, also test the following L1 ratios: [.1, .5, .7, .9, .95, .99] (it is good practice to try more ratio values near the L1 term, as the ridge penalty tends to have higher absolute magnitude).\n",
    "\n",
    "**Do not change `random_state=209`, to facilitate grading.**\n",
    "\n",
    "**6.2** As we used `n_informative = 100`, the true betas will contain 100 non-zero values. Let's see if our estimators picked up on that trend. Print the number of betas greater than $10^{-6}$ (non-zero values) for each estimator, and comment on the results.\n",
    "\n",
    "**6.3**  Let's see how our estimators perform on the test set. Calculate $R^2$ for each estimator on the test set. Comment on the results.\n",
    "\n",
    "**6.4** Now, let's observe what happens when we  increase the number of informative features. Generate another regression problem with the same parameters as before, but this time with an n_informative of 600. Finally, fit OLS, Ridge, LASSO and EN, and print the number of non-zero coefficients and R2 Scores.\n",
    "\n",
    "\n",
    "**6.5**  Compare the results with the previous case and comment. What can we say about LASSO and Elastic Net in particular?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "n= 500\n",
    "p= 2500\n",
    "informative= 100\n",
    "rs = 209\n",
    "sd = 10\n",
    "\n",
    "# Generate regresion\n",
    "X,y,true_coef = make_regression(n_samples = n, n_features = p, n_informative = informative,\n",
    "                                coef = True, noise = sd, random_state=rs)\n",
    "\n",
    "# Get train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1** Use the provided notebook cell to to build a dataset with 500 samples, 2500 features, 100 informative features and a noise sd of 10.0. The function will return the true coefficients in `true_coef`. Intercepts are not generated, so do not fit them in your regressions. Fit LinearRegression, LassoCV, RidgeCV and ElasticNetCV estimators on the traininig set with 5-fold crossvalidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2** As we used `n_informative = 100`, the true betas will contain 100 non-zero values. Let's see if our estimators picked up on that trend. Print the number of betas greater than $10^{-6}$ (non-zero values) for each estimator, and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.3**  Let's see how our estimators perform on the test set. Calculate $R^2$ for each estimator on the test set. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.4** Now, let's observe what happens when we  increase the number of informative features. Generate another regression problem with the same parameters as before, but this time with an n_informative of 600. Finally, fit OLS, Ridge, LASSO and EN, and print the number of non-zero coefficients and R2 Scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.5**  Compare the results with the previous case and comment. What can we say about LASSO and Elastic Net in particular?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 7 [1pt] (for fun) </b></div>\n",
    "\n",
    "We would like to visualize how Ridge, LASSO and Elastic Net behave. We will build a toy regression example to observe the behavior of the coefficients and loss function as lambda increases.\n",
    "\n",
    "**7.1** Use `sklearn.datasets.make_regression` to build a well-conditioned regression problem with 1000 samples, 5 features, noise standard deviation of 10 and random state 209.\n",
    "\n",
    "**7.2** Find the Ridge, LASSO and EN estimator for this problem, varying the regularization parameter in the interval $[0.1,100]$ for LASSO and EN, and $[0.1,10000]$ for Ridge. Plot the evolution of the 5 coefficients for each estimator in a 2D plot, where the X axis is the regularization parameter and the Y axis is the coefficient value. For Elastic Net, make 4 plots, each one with one of the following L1 ratios: $[0.1, 0.5, 0.8, 0.95]$ You should have 6 plots: one for Lasso, one for Ridge, and 4 for EN. Each plot should have 5 curves, one per coefficient. \n",
    "\n",
    "**7.3** Comment on this evolution. Does this make sense with what we've seen so far?\n",
    "\n",
    "**7.4** We're now interested in visualizing the behavior of the Loss functions. First, generate a regression problem with 1000 samples and 2 features. Then, use the provided \"loss_3d_interactive\" function to observe how the loss surface changes as the regularization parameter changes. Test the function with Ridge_loss, LASSO_loss and EN_loss. Comment on what you observe.**\n",
    "\n",
    "**Note: for this to work, you have to install plotly. Go to https://plot.ly/python/getting-started/ and follow the steps. You don't need to make an account as we'll use the offline mode.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.1** Use `sklearn.datasets.make_regression` to build a well-conditioned regression problem with 1000 samples, 5 features, noise standard deviation of 10 and random state 209.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.2** Find the Ridge, LASSO and EN estimator for this problem, varying the regularization parameter in the interval $[0.1,100]$ for LASSO and EN, and $[0.1,10000]$ for Ridge. Plot the evolution of the 5 coefficients for each estimator in a 2D plot, where the X axis is the regularization parameter and the Y axis is the coefficient value. For Elastic Net, make 4 plots, each one with one of the following L1 ratios: $[0.1, 0.5, 0.8, 0.95]$ You should have 6 plots: one for Lasso, one for Ridge, and 4 for EN. Each plot should have 5 curves, one per coefficient. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.3** Comment on this evolution. Does this make sense with what we've seen so far?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.4** We're now interested in visualizing the behavior of the Loss functions. First, generate a regression problem with 1000 samples and 2 features. Then, use the provided \"loss_3d_interactive\" function to observe how the loss surface changes as the regularization parameter changes. Test the function with Ridge_loss, LASSO_loss and EN_loss. Comment on what you observe.**\n",
    "\n",
    "**Note: for this to work, you have to install plotly. Go to https://plot.ly/python/getting-started/ and follow the steps. You don't need to make an account as we'll use the offline mode.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y,true_coef = make_regression(n_samples = 1000, n_features = 2, noise = 10, random_state=209, coef=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interactive, HBox, VBox\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "def OLS_loss(X, y, beta, lbda=0):\n",
    "    y_hat = np.dot(X,beta)\n",
    "    return np.sum((y_hat-y)**2,axis=0)\n",
    "\n",
    "def Ridge_loss(X, y, beta, lbda):\n",
    "    y_hat = np.dot(X,beta)\n",
    "    return np.sum((y_hat-y)**2,axis=0) + lbda*np.sum(beta**2, axis=0)\n",
    "\n",
    "def LASSO_loss(X, y, beta, lbda):\n",
    "    y_hat = np.dot(X,beta)\n",
    "    return (1 / (2 * len(X)))*np.sum((y_hat-y)**2,axis=0) + lbda*np.sum(np.abs(beta), axis=0)\n",
    "\n",
    "def EN_loss(X, y, beta, lbda):\n",
    "    ratio=0.1\n",
    "    y_hat = np.dot(X,beta)\n",
    "    return (1 / (2 * len(X)))*np.sum((y_hat-y)**2,axis=0) + lbda*(ratio*np.sum(beta**2, axis=0) + (1-ratio)*np.sum(np.abs(beta), axis=0))\n",
    "\n",
    "def loss_3d_interactive(X, y, loss='Ridge'):\n",
    "    '''Uses plotly to draw an interactive 3D representation of the loss function, \n",
    "    with a slider to control the regularization factor.\n",
    "    \n",
    "    Inputs:\n",
    "    X: predictor matrix for the regression problem. Has to be of dim n x 2\n",
    "    y: response vector \n",
    "    \n",
    "    loss: string with the loss to plot. Options are 'Ridge', 'LASSO', 'EN'.\n",
    "    '''\n",
    "    \n",
    "    if loss == 'Ridge':\n",
    "        loss_function = Ridge_loss\n",
    "        lbda_slider_min = 0\n",
    "        lbda_slider_max = 10000\n",
    "        lbda_step = 10\n",
    "        clf = Ridge()\n",
    "    elif loss == 'LASSO':\n",
    "        loss_function = LASSO_loss\n",
    "        lbda_slider_min = 1\n",
    "        lbda_slider_max = 150\n",
    "        lbda_step = 1\n",
    "        clf = Lasso()\n",
    "    elif loss == 'EN':\n",
    "        loss_function = EN_loss\n",
    "        lbda_slider_min = 1\n",
    "        lbda_slider_max = 150\n",
    "        lbda_step = 1\n",
    "        clf = ElasticNet()\n",
    "    else:\n",
    "        raise ValueError(\"Loss string not recognized. Available options are: 'Ridge', 'LASSO', 'EN'.\")\n",
    "        \n",
    "    \n",
    "    # linspace for loss surface\n",
    "    L=20\n",
    "    lsp_b = np.linspace(-80,80,L)\n",
    "    lsp_b_x, lsp_b_y = np.meshgrid(lsp_b,lsp_b)\n",
    "    lsp_b_mat = np.column_stack((lsp_b_x.flatten(),lsp_b_y.flatten()))\n",
    "    \n",
    "    # Get all optimal betas for current lambda range\n",
    "    precomp_coefs=[]\n",
    "    for l in range(lbda_slider_min,lbda_slider_max+1,lbda_step):\n",
    "        clf.set_params(alpha=l)\n",
    "        clf.fit(X, y)\n",
    "        precomp_coefs.append(clf.coef_)\n",
    "                \n",
    "    f = go.FigureWidget(\n",
    "        data=[\n",
    "            go.Surface(\n",
    "                    x=lsp_b_x,\n",
    "                    y=lsp_b_y,\n",
    "                    z=loss_function(X,y.reshape(-1,1), lsp_b_mat.T, 0).reshape((L,L)),\n",
    "                    colorscale='Viridis',\n",
    "                    opacity=0.7,\n",
    "                    contours=dict(z=dict(show=True,\n",
    "                                         width=3,\n",
    "                                         highlight=True,\n",
    "                                         highlightcolor='orange',\n",
    "                                         project=dict(z=True),\n",
    "                                         usecolormap=True))\n",
    "            ),\n",
    "            \n",
    "            go.Scatter3d(\n",
    "                x=[p[0] for p in precomp_coefs],\n",
    "                y=[p[1] for p in precomp_coefs],\n",
    "                z=np.zeros(len(precomp_coefs)),\n",
    "                marker=dict(\n",
    "                    size=1,\n",
    "                    color='darkorange',\n",
    "                    line=dict(\n",
    "                        color='darkorange',\n",
    "                        width=1\n",
    "                        ),\n",
    "                    opacity=1\n",
    "                    )\n",
    "                ),\n",
    "            go.Scatter3d(\n",
    "                x=[0],\n",
    "                y=[0],\n",
    "                z=[0],\n",
    "                \n",
    "                marker=dict(\n",
    "                    size=10,\n",
    "                    color='orange',\n",
    "                    opacity=1\n",
    "                    ),\n",
    "            )\n",
    "        ],\n",
    "\n",
    "        layout=go.Layout(scene=go.layout.Scene(\n",
    "                    xaxis = dict(\n",
    "                        title='Beta 1'),\n",
    "                    yaxis = dict(\n",
    "                        title='Beta 2'),\n",
    "                    zaxis = dict(\n",
    "                        title='Loss'),\n",
    "            camera=go.layout.scene.Camera(\n",
    "                up=dict(x=0, y=0, z=1),\n",
    "                center=dict(x=0, y=0, z=0),\n",
    "                eye=dict(x=1.25, y=1.25, z=1.25))\n",
    "        ),\n",
    "            width=1000,\n",
    "            height=700,)\n",
    "    )\n",
    "\n",
    "    def update_z(lbda):\n",
    "        f.data[0].z = loss_function(X, y.reshape(-1,1), lsp_b_mat.T, lbda).reshape((L,L))\n",
    "        beta_opt = precomp_coefs[(lbda-lbda_slider_min)//(lbda_step)]\n",
    "        f.data[-1].x = [beta_opt[0]]\n",
    "        f.data[-1].y = [beta_opt[1]]\n",
    "        f.data[-1].z = [0]\n",
    "\n",
    "    lambda_slider = interactive(update_z, lbda=(lbda_slider_min, lbda_slider_max, lbda_step))\n",
    "    vb = VBox((f, lambda_slider))\n",
    "    vb.layout.align_items = 'center'\n",
    "    display(vb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#youe code here\n",
    "loss_3d_interactive(X, y, loss='Ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#youe code here\n",
    "loss_3d_interactive(X, y, loss='LASSO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#youe code here\n",
    "loss_3d_interactive(X, y, loss='EN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
